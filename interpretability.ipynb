{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "DATA_DIR = '/content/gdrive/MyDrive/data' \n",
    "DATA_DIR = '../data'\n",
    "STUDY_DIR = DATA_DIR + '/study'\n",
    "\n",
    "users = pd.read_csv(f'{DATA_DIR}/users.csv.gz')\n",
    "topics = pd.read_csv(f'{DATA_DIR}/topics_translated.csv')\n",
    "documents = pd.read_csv(f'{DATA_DIR}/documents.csv.gz')\n",
    "\n",
    "# use study for less data, for testing\n",
    "events = pd.read_csv(f'{STUDY_DIR}/events.csv.gz')\n",
    "transactions = pd.read_csv(f'{STUDY_DIR}/transactions.csv.gz')\n",
    "\n",
    "topic_trees = pd.read_csv(f'{DATA_DIR}/topic_trees.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13429/1305080819.py:12: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  topic_trees = topic_trees.append(math_row, ignore_index=True)\n",
      "/tmp/ipykernel_13429/1305080819.py:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  topic_trees = topic_trees.append(german_row, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "math_topics = set(topics[topics['math']==1]['id'])\n",
    "german_topics = set(topics[topics['math']==0]['id'])\n",
    "\n",
    "topic_trees = topic_trees[~topic_trees['parent_id'].isna()]\n",
    "\n",
    "german_row = {'topic_id':0, 'parent_id':0, 'child_id':1, 'sibling_rank':0, 'displayed_on_dashboard': 0}\n",
    "math_row = {'topic_id':1, 'parent_id':0, 'child_id':109, 'sibling_rank':0, 'displayed_on_dashboard': 0}\n",
    "\n",
    "topic_trees = topic_trees.append(math_row, ignore_index=True)\n",
    "topic_trees = topic_trees.append(german_row, ignore_index=True)\n",
    "\n",
    "math_topic_tree = topic_trees[(topic_trees['parent_id'].isin(math_topics)) | (topic_trees['child_id'].isin(math_topics))]\n",
    "german_topic_tree = topic_trees[(topic_trees['parent_id'].isin(german_topics)) | (topic_trees['child_id'].isin(german_topics))]\n",
    "\n",
    "\n",
    "# def draw_topic_tree(topic_tree, title, node_color, figsize):\n",
    "#   G = nx.from_pandas_edgelist(topic_tree, source='child_id', target='parent_id', edge_attr=['topic_id'])\n",
    "#   plt.figure(figsize=figsize)\n",
    "#   options = {\"edge_color\": \"tab:gray\", \"node_color\": node_color, \"node_size\": 100, \"alpha\": 0.8, \"font_size\": 7}\n",
    "#   nx.draw_networkx(G, pos=nx.spring_layout(G), **options)\n",
    "#   plt.title(title)\n",
    "#   plt.show()\n",
    "\n",
    "# draw_topic_tree(german_topic_tree, \"Math topic tree\", 'tab:red', (13, 12))\n",
    "\n",
    "\n",
    "G = nx.from_pandas_edgelist(topic_trees, source='child_id', target='parent_id', edge_attr=['topic_id'])\n",
    "\n",
    "PATHS = dict(nx.all_pairs_shortest_path_length(G, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "transactions = transactions[~transactions['topic_id'].isna()]\n",
    "transactions = transactions[~transactions['user_id'].isna()]\n",
    "\n",
    "transactions['transaction_id'] = transactions['transaction_id'].astype(int)\n",
    "transactions['user_id'] = transactions['user_id'].astype(int)\n",
    "\n",
    "def compute_percentage_correct(transactions_df):\n",
    "\n",
    "    topics = pd.DataFrame({'topic_id': transactions_df['topic_id'].unique(), 'dummy': transactions['topic_id'].unique()})\n",
    "    topics = topics.set_index('topic_id', drop=True)\n",
    "\n",
    "    partial_per_topic = transactions_df[transactions_df['evaluation'] == 'PARTIAL'].groupby('topic_id').count()['transaction_id']\n",
    "    correct_per_topic = transactions_df[transactions_df['evaluation'] == 'CORRECT'].groupby('topic_id').count()['transaction_id']\n",
    "    wrong_per_topic = transactions_df[transactions_df['evaluation'] == 'WRONG'].groupby('topic_id').count()['transaction_id']\n",
    "\n",
    "    ppu_keys = partial_per_topic.keys()\n",
    "    cpu_keys = correct_per_topic.keys()\n",
    "    wpu_keys = wrong_per_topic.keys()\n",
    "\n",
    "    def correctness_score(row):\n",
    "        tid = row.name\n",
    "        \n",
    "        n_wrong = 0 if tid not in wpu_keys else wrong_per_topic[tid]\n",
    "        n_partial = 0 if tid not in ppu_keys else partial_per_topic[tid]\n",
    "        n_correct = 0 if tid not in cpu_keys else correct_per_topic[tid]\n",
    "\n",
    "        total = n_wrong + n_correct + n_partial\n",
    "\n",
    "        score = 100 * (n_correct + 0.5 * n_partial)\n",
    "\n",
    "\n",
    "        if total == 0:\n",
    "            return 0\n",
    "        \n",
    "        score /= total\n",
    "        \n",
    "        return score\n",
    "\n",
    "    return topics.apply(correctness_score, axis=1)\n",
    "\n",
    "TOPIC_CORRECTNESS = compute_percentage_correct(transactions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "# NOTE: param 'topic' should be the topic_id as found in the topics table #\n",
    "###########################################################################\n",
    "\n",
    "\n",
    "def generality(topic, user):\n",
    "    \"\"\"\n",
    "    how general vs how detailed is the topic,\n",
    "    distance in topic tree from root\n",
    "    \"\"\"\n",
    "\n",
    "    assert topic in PATHS[0].keys() and topic != 0, \"topic not found, make sure the topic id is in the original topics table\"\n",
    "\n",
    "    return 1/PATHS[0][topic]\n",
    "\n",
    "    \n",
    "\n",
    "def difficulty(topic, user):\n",
    "    \"\"\"\n",
    "    difficulty of the topic,\n",
    "    fraction of questions that were answered incorrectly \n",
    "    \"\"\"\n",
    "    \n",
    "    if topic in TOPIC_CORRECTNESS.keys():\n",
    "        return (100 - TOPIC_CORRECTNESS[topic])/100 \n",
    "\n",
    "    return 0.5\n",
    "\n",
    "def novelty(topic, user):\n",
    "    \"\"\"\n",
    "    novelty of the topic,\n",
    "    1 = new, 0 = old\n",
    "    \"\"\"\n",
    "    \n",
    "    assert topic in topics['id'].unique() and user in events['user_id'].unique(), \"MESSAGE TODO:\"\n",
    "\n",
    "    user_topics = events[events['user_id'] == user]['topic_id'].unique()\n",
    "\n",
    "    return int(topic in user_topics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: vary the 'gender' feature for all the datapoints in the test set and compare recommended topics\n",
    "\n",
    "n = 5\n",
    "\n",
    "# TODO: replace this with the output from the NCF feature model\n",
    "example_output = {\n",
    "    0: {'Male': [0,1,2,3,4], 'Female': [4,5,6,7,8], 'Other': [5,6,7,8,9]},\n",
    "    1: {'Male': [0,1,2,3,4], 'Female': [4,5,6,7,8], 'Other': [5,6,7,8,9]},\n",
    "    2: {'Male': [0,1,2,3,4], 'Female': [4,5,6,7,8], 'Other': [5,6,7,8,9]},\n",
    "}\n",
    "\n",
    "L = n * len(example_output.keys())\n",
    "\n",
    "\n",
    "metrics = ['generality', 'difficulty', 'novelty']\n",
    "\n",
    "\n",
    "func = {'generality': generality, 'difficulty': difficulty, 'novelty': novelty}\n",
    "\n",
    "male = {'generality': 0, 'difficulty': 0, 'novelty': 0}\n",
    "female = {'generality': 0, 'difficulty': 0, 'novelty': 0}\n",
    "other = {'generality': 0, 'difficulty': 0, 'novelty': 0}\n",
    "\n",
    "for uid in example_output.keys():\n",
    "    outputs = example_output[uid]\n",
    "\n",
    "    for i in range(n):\n",
    "        for k in metrics:\n",
    "            male[k] += func[k](outputs['Male'][i], uid)/L\n",
    "            female[k] += func[k](outputs['Female'][i], uid)/L\n",
    "            other[k] += func[k](outputs['Other'][i], uid)/L\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "metrics_labels = [\"Generality\", \"Difficulty\", \"Novelty\"]\n",
    "data = {\n",
    "    'Female': [female[k] for k in metrics],\n",
    "    'Male': [male[k] for k in metrics],\n",
    "    'Other': [other[k] for k in metrics],\n",
    "}\n",
    "\n",
    "x = np.arange(len(metrics_labels))  # the label locations\n",
    "width = 0.25  # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "fig, ax = plt.subplots(layout='constrained')\n",
    "\n",
    "for attribute, measurement in data.items():\n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x + offset, measurement, width, label=attribute)\n",
    "    ax.bar_label(rects, padding=3)\n",
    "    multiplier += 1\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Value')\n",
    "ax.set_title('Recommended topic metrics, variation based on gender')\n",
    "ax.set_xticks(x + width, metrics_labels)\n",
    "ax.legend(loc='upper left', ncols=3)\n",
    "ax.set_ylim(0, 250)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: for all models, compute those metrics for the predictions on the test set\n",
    "# compare the models, prediction of novelty, difficulty, generality\n",
    "\n",
    "\n",
    "n = 5\n",
    "\n",
    "# TODO: replace this with the output from the different models\n",
    "example_output = {\n",
    "    0: {'NCF': [0,1,2,3,4], 'NCF with features': [4,5,6,7,8], 'GRU4Rec': [5,6,7,8,9]},\n",
    "    1: {'NCF': [0,1,2,3,4], 'NCF with features': [4,5,6,7,8], 'GRU4Rec': [5,6,7,8,9]},\n",
    "    2: {'NCF': [0,1,2,3,4], 'NCF with features': [4,5,6,7,8], 'GRU4Rec': [5,6,7,8,9]},\n",
    "}\n",
    "\n",
    "L = n * len(example_output.keys())\n",
    "\n",
    "\n",
    "metrics = ['generality', 'difficulty', 'novelty']\n",
    "\n",
    "\n",
    "func = {'generality': generality, 'difficulty': difficulty, 'novelty': novelty}\n",
    "\n",
    "ncf = {'generality': 0, 'difficulty': 0, 'novelty': 0}\n",
    "ncf_feature = {'generality': 0, 'difficulty': 0, 'novelty': 0}\n",
    "gru4rec = {'generality': 0, 'difficulty': 0, 'novelty': 0}\n",
    "\n",
    "for uid in example_output.keys():\n",
    "    outputs = example_output[uid]\n",
    "\n",
    "    for i in range(n):\n",
    "        for k in metrics:\n",
    "            ncf[k] += func[k](outputs['NCF'][i], uid)/L\n",
    "            ncf_feature[k] += func[k](outputs['NCF with features'][i], uid)/L\n",
    "            gru4rec[k] += func[k](outputs['GRU4Rec'][i], uid)/L\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "metrics_labels = [\"Generality\", \"Difficulty\", \"Novelty\"]\n",
    "data = {\n",
    "    'NCF': [ncf[k] for k in metrics],\n",
    "    'NCF with features': [ncf_feature[k] for k in metrics],\n",
    "    'GRU4Rec': [gru4rec[k] for k in metrics],\n",
    "}\n",
    "\n",
    "x = np.arange(len(metrics_labels))  # the label locations\n",
    "width = 0.25  # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "fig, ax = plt.subplots(layout='constrained')\n",
    "\n",
    "for attribute, measurement in data.items():\n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x + offset, measurement, width, label=attribute)\n",
    "    ax.bar_label(rects, padding=3)\n",
    "    multiplier += 1\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Value')\n",
    "ax.set_title('Recommended topic metrics, comparison of different models')\n",
    "ax.set_xticks(x + width, metrics_labels)\n",
    "ax.legend(loc='upper left', ncols=3)\n",
    "ax.set_ylim(0, 250)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement LIME-RS for the models, look at some specific test examples and compare"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlbd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
